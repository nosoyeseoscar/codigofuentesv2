---
title: "Instalando DeepSeek en local con Windows con Ollama y Docker"
description: "F치cil y sencillo sin WSL"
pubDate: "2025-03-24T20:09:49.053Z"
heroImage: "/blogImages/ollama-deepseek.webp"
categories: ['Inteligencia Artificial', 'IA']
tags: ['Ollama', 'LLM', 'DeepSeek', 'Windows', 'Docker']
author: "Oscar Fuentes"
---

## Sin morir en el intento

Como parte del **plan maestro** que ya les hab칤a contado, el siguiente paso individual es instalar un **LLM en local**. Ya saben, para mantener todo privado y evitar que datos a nuestro resguardo anden volando por la red.

Y s칤, la forma m치s sencilla es instalar **LM Studio** y desde ah칤 hacer todo, pero como nos gusta usar la **terminal de comandos** para creernos 칠lites, aqu칤 te dejo una forma que hasta el momento cumple con dos cosas: **no es complicada** y **no tan invasiva** (ahora explico esto).

Si optas por la forma sencilla (**LM Studio**), notar치s que si bien tiene una interfaz gr치fica ya incluida y expone un **EndPoint** (que es lo que buscamos para nuestro plan maestro), siempre est치 cargada en RAM. Es decir, aparece su 칤cono en nuestra 치rea de notificaciones (**System Tray**) como un demonio siempre corriendo. Lo anterior no es malo *per se*, pero si nuestro proyecto no ocupa una interfaz gr치fica como la que vemos al entrar a las p치ginas oficiales de los LLM, o nos gustar칤a escoger alguna que no es la que LM Studio nos da por *default*, usaremos el m칠todo que te voy a explicar a continuaci칩n.

---

## Requerimientos previos

- Voy a suponer que usas **Windows**.
- Que ya tienes instalado **Docker** y tienes al menos una noci칩n b치sica de lo que son los contenedores.
- Tienes suficiente espacio en disco para contener el modelo que escojas. En este ejemplo, vamos a usar la versi칩n destilada **DeepSeek R1 7B** (que pesa **4.7GB**).

---

## Instalaci칩n de Ollama en Windows

El primer paso es instalar **Ollama**. Por suerte, ya existe versi칩n para nuestro sistema operativo y no tienes que hacer malabares usando el **subsistema de Windows para Linux (WSL)** para instalarlo. Nos dirigimos a:

游댕 [Descargar Ollama para Windows](https://ollama.com/download/windows)

Y pues ya sabes: **descargas, instalas, siguiente, siguiente y listo.**

Ver치s un peque침o 칤cono de una llama en el 치rea de notificaciones. Para asegurarnos de que est치 instalado correctamente, abrimos una terminal y tecleamos:

```sh
ollama
```

Si todo sali칩 bien, nos tiene que regresar informaci칩n del programa y no un error.

---

## Descarga del modelo LLM

Ahora tenemos que bajar el modelo LLM que vamos a instalar. Como mencion칠 antes, puedes elegir qu칠 modelo bajas y qu칠 versi칩n usar치s. Ten en cuenta que muchos son los factores a considerar, como el hardware con el que cuentes.

Si eliges mal el modelo, es posible que tengas que cambiar a otro m치s ligero o m치s pesado. **Mientras m치s completo est칠 el modelo (y m치s pese), mejores respuestas obtendr치s**, pero si no tienes suficiente **RAM y poder de c칩mputo**, puede que tus consultas sean lentas o fallen.

En este caso, vamos a usar **DeepSeek R1 7B (4.7GB)**. Para instalarlo, ejecutamos el siguiente comando en la terminal:

```sh
ollama run deepseek-r1
```

Esperamos a que descargue los archivos necesarios. Dependiendo de tu conexi칩n a Internet, puede tardar un poco.

Una vez terminado el proceso, **Ollama** nos mostrar치 un mensaje similar a:

```sh
>>> Send a message (/? for help)
```

Podemos probarlo escribiendo un simple **"hola"**, y veremos c칩mo el modelo nos responde amablemente que est치 listo para trabajar. 游꿀

---

## Mejorando la interfaz de usuario con Open WebUI

Si bien **Ollama** ya est치 funcionando, usarlo directamente en la terminal es algo **limitado y poco amigable**. Por eso, vamos a instalar una **interfaz gr치fica** que nos permita usarlo de una manera m치s visual.

Ojo, no hagas lo siguiente, pero si necesitaras salir de Ollama, usa:

```sh
/bye
```

### Instalando Open WebUI con Docker

Ahora vamos a usar **Docker**. Si no sabes qu칠 es Docker, te recomiendo ver alg칰n video explicativo sobre contenedores.

Usaremos **Open WebUI**, una plataforma de IA **autohospedada** que corre completamente **offline** y es compatible con varios ejecutores LLM, entre ellos **Ollama**. Es la opci칩n por *default*, y adem치s, permite usar un motor **RAG** para alimentar el LLM con datos externos.

游닀 [Documentaci칩n de Open WebUI](https://docs.openwebui.com/)

La instalaci칩n se hace con el siguiente comando:

```sh
docker run -d -p 3000:8080 --add-host=host.docker.internal:host-gateway -v open-webui:/app/backend/data --name open-webui --restart always ghcr.io/open-webui/open-webui:main
```

Si bien este comando tiene varios par치metros de *desarrollo* que pueden ser peligrosos, con esto cargamos un contenedor que corre en **Docker**. El comando nos regresar치 un **hash** que identifica el contenedor creado.

Para acceder a la interfaz, abrimos un navegador y entramos a:

游댕 [http://localhost:3000](http://localhost:3000)

Si tu m치quina no es muy potente, la interfaz puede tardar un poco en activarse y podr칤as recibir un **error 500**. Pero **no te preocupes**, el sitio se levantar치 en unos instantes.

游댳 **Nota:** Si los puertos **3000** o **8080** ya est치n ocupados en otros proyectos, puedes cambiarlos en el comando de Docker.

---


Esta interfaz tambi칠n te permite **escoger entre diferentes modelos** si decides instalar m치s de uno, facilitando las pruebas y experimentaci칩n.

Ahora ya puedes **divertirte localmente y en privado** con tu LLM sin mucho esfuerzo, solo necesitar치s esperar un poco si tu Internet no es muy r치pido. 游

춰**Happy ... 쯣rompting?** 游꿀

